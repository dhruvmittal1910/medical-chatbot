{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e8557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30809bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\medical-chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e277ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\projects\\\\medical-chatbot'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17266e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader,DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da8cc55",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06dc3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf_files(data_path):\n",
    "    # here in data folder you can add more pdfs too, loader will load all of them\n",
    "    loader=DirectoryLoader(\n",
    "        data_path,glob=\"*.pdf\",loader_cls=PyPDFLoader\n",
    "    )\n",
    "    \n",
    "    documents=loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6506ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=extract_pdf_files(\"data\")\n",
    "# extracted_data\n",
    "# langchain returning the document format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f9f88e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "637"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data) #number of pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facc0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page content is needed and it is present in document that we get\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs:List[Document])->List[Document]:\n",
    "    \"\"\"\n",
    "    Given a list of documents, i wnat to return a new list of documents that only contains the source and the page_content\n",
    "    \"\"\"\n",
    "    \n",
    "    minimal_docs:List[Document]=[]\n",
    "    for doc in docs:\n",
    "        src=doc.metadata.get('source')\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\":src}\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    return minimal_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e553a8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_text=filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693dff7",
   "metadata": {},
   "source": [
    "# chunkning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "615167f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(extracted_text):\n",
    "    # here create the chunks of the extracted data\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=20,\n",
    "        # length_function=len\n",
    "    )\n",
    "    \n",
    "    chunked_text=text_splitter.split_documents(extracted_text)\n",
    "    return chunked_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a51f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_splitter(extracted_text))\n",
    "texts_chunk=text_splitter(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f292e2",
   "metadata": {},
   "source": [
    "# do embeddings of the chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f4e1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hugging face embedding model\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# download the pretrained embedding model we want to use\n",
    "\n",
    "def download_embedding_model():\n",
    "    \"\"\"\n",
    "    Download the model and return the hugging face embedddings\n",
    "    \"\"\"\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings=HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31ca159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5f73a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the pinecone and open ai api key  \n",
    "PINECONE_API_KEY=os.getenv('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "os.environ['PINECONE_API_KEY']=PINECONE_API_KEY\n",
    "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc3b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
